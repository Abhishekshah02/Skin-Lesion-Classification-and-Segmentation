{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3287d623",
   "metadata": {},
   "source": [
    "# Skin lesion Classification and segementation using the pretrianed model RESNET 50 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee000c5",
   "metadata": {},
   "source": [
    "Importing the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be1a2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95f42f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir = r\"C:\\Users\\Amrit Shah\\Desktop\\Minor Project\\Skin-Lesion-Classification-and-Segmentation\\train\"\n",
    "train_metadata_path = r\"C:\\Users\\Amrit Shah\\Desktop\\Minor Project\\Skin-Lesion-Classification-and-Segmentation\\train_metadata.csv\"\n",
    "val_image_dir = r\"C:\\Users\\Amrit Shah\\Desktop\\Minor Project\\Skin-Lesion-Classification-and-Segmentation\\val\"\n",
    "val_metadata_path = r\"C:\\Users\\Amrit Shah\\Desktop\\Minor Project\\Skin-Lesion-Classification-and-Segmentation\\val_metadata.csv\"\n",
    "test_image_dir = r\"C:\\Users\\Amrit Shah\\Desktop\\Minor Project\\Skin-Lesion-Classification-and-Segmentation\\test\"\n",
    "test_metadata_path = r\"C:\\Users\\Amrit Shah\\Desktop\\Minor Project\\Skin-Lesion-Classification-and-Segmentation\\test_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "303bbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_image_label(image_path, dx):\n",
    "    try:\n",
    "        image_data = tf.io.read_file(image_path)\n",
    "        image_data = tf.image.decode_jpeg(image_data, channels=3)  # Load image\n",
    "        image_data = tf.image.resize(image_data, [256, 256])       # Ensure it is 256x256\n",
    "        image_data = tf.image.convert_image_dtype(image_data, tf.float32)  # Normalize to [0, 1]\n",
    "        dx = tf.cast(dx, tf.int32)\n",
    "        return image_data, dx\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return tf.zeros([256, 256, 3], dtype=tf.float32), tf.constant(-1, dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73d04735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(image_dir, metadata_path, batch_size=32, shuffle=True, sample_count=None):\n",
    "    df = pd.read_csv(metadata_path)\n",
    "\n",
    "    if sample_count is not None:\n",
    "        df = df.iloc[:sample_count]\n",
    "\n",
    "    # Automatically create label mapping\n",
    "    if df['dx'].dtype == object:\n",
    "        unique_labels = sorted(df['dx'].unique())\n",
    "        label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        df['dx'] = df['dx'].map(label_mapping)\n",
    "\n",
    "    image_paths = df['image_id'].apply(lambda x: os.path.join(image_dir, x + \".jpg\")).tolist()\n",
    "    labels = df['dx'].tolist()\n",
    "    ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    ds = ds.map(lambda path, dx: load_image_label(path, dx),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=sample_count or len(df), seed=42)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99c841c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "train_ds = create_dataset(train_image_dir, train_metadata_path, batch_size=batch_size, shuffle=True, sample_count=12000)\n",
    "val_ds   = create_dataset(val_image_dir, val_metadata_path, batch_size=batch_size, shuffle=False, sample_count=6000)\n",
    "test_ds  = create_dataset(test_image_dir, test_metadata_path, batch_size=batch_size, shuffle=False, sample_count=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "968e4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    # Reduced brightness and contrast augmentation\n",
    "    image = tf.image.random_brightness(image, max_delta=0.05)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    # Limit rotation to 0° or 90° (instead of 0°, 90°, 180°, 270°)\n",
    "    k = tf.random.uniform(shape=[], minval=0, maxval=2, dtype=tf.int32)\n",
    "    image = tf.image.rot90(image, k)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55439866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: (32, 256, 256, 3)\n",
      "DX batch shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check a batch's shapes\n",
    "for img_batch, dx_batch in train_ds.take(1):\n",
    "    print(f\"Image batch shape: {img_batch.shape}\")   # Expected: (32, 256, 256, 3)\n",
    "    print(f\"DX batch shape: {dx_batch.shape}\")         # Expected: (32,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3dc9c",
   "metadata": {},
   "source": [
    "Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b2e43f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_7      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_7      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m524,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m1,799\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,114,055</span> (91.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,114,055\u001b[0m (91.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">526,343</span> (2.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m526,343\u001b[0m (2.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "num_classes=7\n",
    "\n",
    "# Load pre-trained ResNet50\n",
    "base_model = ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(256, 256, 3)\n",
    ")\n",
    "\n",
    "# Freeze base layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom head\n",
    "def resnet(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "input_shape = (256, 256, 3)\n",
    "resnet_model = resnet(input_shape, num_classes)\n",
    "resnet_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "64d1fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resnet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b610f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Learning Rate Scheduler callback to reduce LR when validation loss plateaus\n",
    "# ---------------------------------------\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-06)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b617d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1155s\u001b[0m 3s/step - accuracy: 0.4356 - loss: 1.4907 - sparse_categorical_accuracy: 0.4356 - val_accuracy: 0.6360 - val_loss: 0.9705 - val_sparse_categorical_accuracy: 0.6360 - learning_rate: 5.0000e-04\n",
      "Epoch 2/2\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1133s\u001b[0m 3s/step - accuracy: 0.6001 - loss: 1.0420 - sparse_categorical_accuracy: 0.6001 - val_accuracy: 0.6607 - val_loss: 0.8892 - val_sparse_categorical_accuracy: 0.6607 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 2\n",
    "history = resnet_model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[early_stopping, reduce_lr], \n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
